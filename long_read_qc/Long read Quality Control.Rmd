--- 
title: "Long read Quality Control"
author: "Matthew Gemmell & Helen Hipperson"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# Overview
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/well.png", auto_pdf = TRUE)
``` 

This practical session aims to introduce you to Long read Quality Control. The topics covered are:

- Acquiring the workshop data
- Example of PacBio quality control
- Example of ONT quality control
- Practice data
- Further resources

<!--chapter:end:01-Long_read_QC.Rmd-->

# Data
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/Play.png", auto_pdf = TRUE)
``` 

## The data

The data for today is in the \"QC_workshop\" directory that you copied into your home directory (\"~\") in the Illumina QC day.

Check you have the directory.

```{bash, eval=FALSE}
ls ~
```

To add some more recently added PacBio files to yuor home directory run the following command:

```{bash eval=FALSE}
cp -r /pub39/tea/matthew/NEOF/QC_workshop/PB_QC ~/QC_workshop/PB_QC
```

Or if you do not have the \"QC_workshop\" directory at all the command to copy it all to your home directory is below.

```{bash eval=FALSE}
cp -r /pub39/tea/matthew/NEOF/QC_workshop/ ~
```

<!--chapter:end:02-Intro.Rmd-->

# PacBio

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/Pacbio_logo.png", auto_pdf = TRUE)
``` 

In this section we will quality check PacBio sequencing raw data (BAM files), perform a filter to retain only the longest reads and output the data in fastq format.

## File formats
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/files.png", auto_pdf = TRUE)
``` 

Sequence data from PacBio are output as subreads in unaligned BAM format. (Further explanation of the BAM format is in section 4.1.3).

The figure below shows a schematic of a circular DNA molecule of a PacBio library with the colours representing:

- Black, I = Insert DNA, i.e\. the double-stranded DNA of your sample
- Red, B = Barcodes, optional single (Left or Right) or double (Left & Right) for multiplexing samples
- Green, A = Adapter, SMRTbell adapters with a double-stranded stem and single-stranded hairpin loop.

```{r, fig.align = 'center',out.width= '90%', echo=FALSE }
knitr::include_graphics(path = "figures/smrtbell.png", auto_pdf = TRUE)
``` 

Sequencing follows the direction of the arrows and continues around until either the end of the sequencing run or if the DNA polymerase fails.

A ZMW read consists of the data collected from a single ZMW, with the HQ (high quality) region recorded when just a single molecule of DNA is present in the ZMW.

```{r, fig.align = 'center',out.width= '90%', echo=FALSE }
knitr::include_graphics(path = "figures/zmw.png", auto_pdf = TRUE)
``` 

This schematic shows the structure of a ZMW read in linear format. Insert DNA subreads are interspersed with the barcode and adapter sequence of the SMRTbell library.
For more information see: https://pacbiofileformats.readthedocs.io/en/10.0/Primer.html

__subreads.bam file__

This contains the sequence for each read (or pass) of the insert DNA (grey).

__scraps.bam file__

This contains the adapter (green) and barcode (red) sequences, as well as any low quality (black) regions.

These BAM files will usually have an accompanying PacBio BAM index file (subreads.bam.pbi and scraps.bam.pbi).

PacBio data can be generated in two ways depending on the goal of the experiment.

- __Continuous Long Reads (CLR)__ the aim is to produce sequence reads as long as possible, such as for genome assembly, sequencing through long repeat regions or finding other large structural variants. CLR runs usually generate one subread (i.e\. there would be just one or one and a bit of the grey insert regions shown above) and sequences have an error rate of ~ 10-15%.

- __Circular Consensus Sequences (CCS)__ the aim is to sequence shorter molecules (up to 10-15kb max), such as amplicons, and to generate accurate consensus sequences. CCS runs generate higher-accuracy sequences from a consensus of many subreads (i.e\. there would be several of the grey insert regions shown above in every ZMW read).

For this tutorial we will carry out quality checking and control on both CLR and CCS BAM files.

```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/toolbox.png", auto_pdf = TRUE)
``` 
__SequelTools__

https://github.com/ISUgenomics/SequelTools

We will use the SequelTools program to both assess and filter our PacBio CLR and CCS data.

__smrttools__

https://www.pacb.com/wp-content/uploads/SMRT_Tools_Reference_Guide_v90.pdf

We will also use smrttools from PacBio to generate any missing index files and CCS reads from subreads BAM files, plus convert BAM to fasta/q format.

__RabbitQC__

https://github.com/ZekunYin/RabbitQC

Lastly, we will also use RabbitQC to assess the CCS assembled reads in fastq format.

## Continuous Long Reads
```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/clr.png", auto_pdf = TRUE)
``` 

Before carrying out any specific commands we will first move into the relevant directory.

```{bash eval=FALSE}
cd ~/QC_workshop/PB_QC/data/CLR
```

Use `ls` to list the contents of this directory. You will see that there are BAM subreads and scraps files for three samples, plus their BAM index files. These data were generated from high molecular weight DNA for _de novo_ genome assembly.

We will use the SequelTools program to both assess and filter our PacBio CLR data. First we need to make a file of filenames (fofn) for the subreads files. You can do this with the following commands:

```{bash eval=FALSE}
find $(pwd) -name "*subreads.bam" | sort > CLR_subreads.txt
```

This command line will find all of the files in our current working directory whose names end in \'subreads.bam\', sorts them alphanumerically and prints these names into a new text file.

### Quality check
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/qc_inspection.png", auto_pdf = TRUE)
``` 

The SequelTools program has three different tools, specified with the `-t` argument, which are:

- `Q` for quality control
- `S` for subsampling the data
- `F` for filtering the data

We will first use `Q` to assess our data. Other options used are:

- `-u` to specify the file containing the list of subread .bam files
- `-o` to specify the name of an output folder for the plots
- `-p` to specify which plots to produce: `b` for a few basic plots, `i` includes a few more detailed plots and `a` generates all available plots.

```{bash eval=FALSE}
SequelTools.sh -t Q -p a -u CLR_subreads.txt -o CLR_QC
```

When this has finished running the CLR_QC folder will contain a \'summaryTable.txt\' file with values on the number and lengths of sequence reads for all three samples, plus a series of plots saved as pdf files. We can use firefox to view the pdf plots.

```{bash eval=FALSE}
firefox CLR_QC/totalBasesBarplot.pdf &
```

This shows the total amount of sequence data for each of the three samples, both for all of the subreads present and \'longestSubs\' (the longest subread within each CLR). We can see that Sample2 has the largest amount of data, and for all three samples most of the data is contained within the longest subreads. This is expected with CLR data - long fragments of DNA are extracted for sequenicng and we often achieve just a single pass of this insert during the PacBio sequenicng run.

There are also plots for the N50 and L50 of each sample:

```{bash eval=FALSE}
firefox CLR_QC/n50s.pdf CLR_QC/l50s.pdf &
```

- N50 = the median sequence length (in bp) of the data; 50% of the sequence data generated is in subreads equal to or greater than this length.
- L50 = the minimum number of subreads whose length makes up the N50 value.

We can see that the N50 is larger for Sample3 compared to Sample1 and Sample2, suggesting the subreads are longer for Sample3. Cnversely, the L50 is higher for Sample2, suggesting the subreads are shorter for this sample as more subreads are required to make up the N50 value.

We can see information on the subread lengths in more detail as boxplots and histograms in the following plots:

```{bash eval=FALSE}
firefox CLR_QC/subreadSizesBoxplots.pdf CLR_QC/*Hists.pdf &
```

The boxplots show that Sample3 does indeed have the longest subread lengths, and highest N50 as indicated by the blue diamond. Sample2 has a larger range of subread lengths than Sample1, but has a slightly lower median length and N50 value.

The histograms show the distribution of subread lengths in more detail. Sample3 shows a big spike of very short subreads - why do you think this is? (We\'ll come back to this after the filtering step!)

### Filtering
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_filtering.png", auto_pdf = TRUE)
``` 

Our quality check of the PacBio data doesn\'t include any assessment of sequence quality. Unaligned PacBio data doesn\'t have a quality score in the same way as the Illumina data we looked at on Tuesday. Quality scores for PacBio are generated when the reads are aligned - either to a reference sequence or subreads aligned to each other to generate CCS reads. However, it is often a good idea to use just the longest PacBio reads for a _de novo_ assembly.

To filter the data by minimum CLR length we will use SequelTools option `-t F`.

Other options used are:

- `-u` to specify the file containing the list of subread .bam files
- `-c` to specify the file containing the list of scraps .bam files
- `-C` to specify to filter by minimum CLR length
- `-Z` to specify the minimum length to keep a CLR
- `-f` to specify the format o the output files: `s`=sam, `b`=bam, `2`=both
- `-o` to specify the name of an output folder for the filtered data files

First we need to make a fofn for the scraps.bam files, as these are needed when using the filtering tool:
```{bash eval=FALSE}
find $(pwd) -name "*scraps.bam" | sort > CLR_scraps.txt
```

Then we can run the filtering command. Here we will keep only CLRs of at least 10,000 bp:
```{bash eval=FALSE}
SequelTools.sh -t F -u CLR_subreads.txt -c CLR_scraps.txt \
-C -Z 10000 -f b -o filtered
```

When this has finished running the \'filtered\' folder will contain filtered BAM files for all three samples. Move into this folder using `cd`:
```{bash eval=FALSE}
cd filtered
```

From here let\'s run the quality control tool on the filtered files and compare the plots to those from the raw files. First we need a new fofn for the filtered subread files:
```{bash eval=FALSE}
find $(pwd) -name "*subreads.bam" | sort > filt_subreads.txt
```

Then run the quality control tool again:
```{bash eval=FALSE}
SequelTools.sh -t Q -p a -u filt_subreads.txt -o filt_QC
```

When this has finished running the \'filt_QC\' folder will contain a text file of summary values and a series of plots saved as pdf files. We can view these as before, for example:
```{bash eval=FALSE}
firefox filt_QC/subreadSizesBoxplots.pdf &
```

And compare this to the boxplots from the unfiltered data:
```{bash eval=FALSE}
firefox ~/QC_workshop/PB_QC/data/CLR/CLR_QC/subreadSizesBoxplots.pdf &
```

The difference looks quite subtle on the plots as the size range of reads is large, but the subread N50 value (indicated by the blue diamond) has increased by ~2,000 bp in the filtered data for Sample2 and Sample3.

There are also still subreads present in the data that are < 10,000 bp. This is because we have filtered out CLRs that are < 10,000 bp, but a CLR > 10,000bp can still be made up of subreads smaller than this.

This is definitely the case for Sample1 and Sample2, where the DNA fragment length of the library was shorter than the CLR lengths, hence there are subreads < 10,000 bp within longer CLRs.

Sample3 shows a more obvious difference before and after filtering:
```{bash eval=FALSE}
firefox ~/QC_workshop/PB_QC/data/CLR/CLR_QC/Sample3.readLenHists.pdf \
filt_QC/Sample3_flt.readLenHists.pdf &
```

For this sample the DNA fragment length of the library was much greater than 10,000 bp, and so the majority of CLRs consist of just one pass of this long insert. The large number of short subreads remaining in the filtered file (blue bars) represent subreads only partially sequenced on the \'return journey\' of the circular library DNA molecule. These are not present when we look only at the longest subread present in each CLR (green bars).

SequelTools doesn\'t have an option for filtering subreads by length. To do this we can convert the filtered BAM file to a fastq or fasta file and use another downstream tool. The fastq file will not contain any useful information on sequence quality (every base is assigned a quality character of \'!\' which is equal to zero), but this file format is useful if only fastq format is accepted as input for a downstream tool.

To do this use the commands `pbindex` and `bam2fastq` from the smrttools package:
```{bash eval=FALSE}
pbindex Sample1_flt.subreads.bam
```

Note that you may see a long warning message ending with \"No such file or directory\". However, if you run `ls` you should see that the output file has been generated.

This command generates a .pbi index file that is needed before we can convert the BAM file to fastq format. The `bam2fastq` command requires an output file prefix name, specified by `-o`, and the name of the BAM file we want to convert:
```{bash eval=FALSE}
bam2fastq -o Sample1_filtered Sample1_flt.subreads.bam
```

You should now have a compressed fastq file for the CLR-filtered subreads from Sample1.

## Circular Consensus Sequences
```{r, fig.align = 'center',out.width= '45%', echo=FALSE }
knitr::include_graphics(path = "figures/ccs.png", auto_pdf = TRUE)
``` 

We will now look at some long amplicon sequencing data. These data were generated from ~4 kb PCR amplicons of a mammalian gene region.

Before carrying out any specific commands we will first move into the relevant directory.

```{bash eval=FALSE}
cd ~/QC_workshop/PB_QC/data/CCS
```

This folder contains one file of subreads from the ampicons sequenicng run, named \'Sample4.subreads.bam\'. Even though we just have one file we want to QC, SequelTools still requires a fofn to run:
```{bash eval=FALSE}
find $(pwd) -name "*subreads.bam"  > subreads.txt
```

```{bash eval=FALSE}
SequelTools.sh -t Q -p a -u subreads.txt -o S4QC
```

When these commands have run you will have a \'S4QC\' folder containing the \'summaryTable.txt\' file the series of plots saved as pdf files. Let\'s have a look at the subread length histogram:
```{bash eval=FALSE}
firefox S4QC/Sample4.readLenHists.pdf &
```

The majority of the longest subreads (green bars) are ~ 4kb (the length of our amplicon), but there are also lots of shorter reads. To filter and further process these data we will use the `ccs` tool from smrttools package.

Options to specify for this tool are:

- `--minLength` to specify the minimum subread length to use to generate a ccs read
- `--minPasses` to specify the minimum number of subreads per ZMW read to generate a ccs read
- `-j` to specify the number of threads to use

Followed by the input filename (the subreads.bam) and the name you want to call the output.

The full command to generate CCS reads is:

          `ccs --minLength 3500 --minPasses 7 -j 1 Sample4.subreads.bam S4_ccs.bam`

However, even though this is a heavily downsized data set this command still takes ~20 minutes to run. To avoid you having to wait we have included the results file already in your directory. If you have typed in the above command to run you can kill it by pressing Ctrl - C.

To see the results summary:
```{bash eval=FALSE}
cd ccs_results
less ccs_report.txt
```

Questions:

- What percentage of the subreads generated a CCS?
- For what reasons did the rest of the subreads fail to generate a CCS?

```{r, fig.align = 'center',out.width= '30%', echo=FALSE }
knitr::include_graphics(path = "figures/rabbit_qc.png", auto_pdf = TRUE)
``` 

Lastly, let\'s perform a quality check on the \'ccs.bam\' file to assess the lengths of the ccs reads. SequelTools only works with the raw subread BAM files, so we will use an alternative program - RabbitQC. However, this program needs a fastq file as input so we will first generate that using the `bam2fastq` tool from smrttools.

The `bam2fastq` command requires an output file prefix name, specified by `-o`, and the name of the BAM file we want to convert. We\'ll also include the option `-u` to give an uncompressed fastq output:
```{bash eval=FALSE}
bam2fastq -u -o S4_ccs S4_ccs.bam
```

We\'re now ready to run RabbitQC. The options to use are:

- `-i` to specify the input fastq file name
- `-D` to specify these are long-read data (i.e\. not Illumina)
- `-w` to specify the number of threads
- `-h` to specify a name for the html results file

Run the following command and then view the output using firefox:
```{bash eval=FALSE}
rabbit_qc -i S4_ccs.fastq -D -w 1 -h S4_ccs_qc.html
```

```{bash eval=FALSE}
firefox S4_ccs_qc.html &
```

The \"Reads Length Distribution\" shows all but three ccs reads to be > 4,000bp. The majority are ~ 4.4kb - in fact the amplicon is a little longer than 4kb plus the reads contain an index sequence added during PCR to allow multiplexing of multiple samples, but this tight distribution of sizes is a good indication that the amplifcation and sequencing has worked well.

The other plots show the % nucleotide frequencies and mean quality scores for the beginning and end of the reads. These aren\'t especially useful as such a short region is shown, but we can see the quality of the ccs reads is very high, and the main advantage of RabbitQC is that it is a very fast way to assess the processed read lengths.

<!--chapter:end:03-PB.Rmd-->

# ONT
```{r, fig.align = 'center',out.width= '60%', echo=FALSE }
knitr::include_graphics(path = "figures/ONT_logo.png", auto_pdf = TRUE)
``` 

In this section we are going to carry out a quick QC of ONT data using the tool suite NanoPack and the tool Porechop. For this we will use the fastq files.

## File formats
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/files.png", auto_pdf = TRUE)
``` 

Although we will only be using fastq files for this tutorial it is important to know the other file formats you may encounter when working with ONT data.

### Fast5
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/fast5.png", auto_pdf = TRUE)
``` 

The raw data from ONT machines come as Fast5 (.fast5) files. This contains the signal data from the pore which can be processed into more useful files.

Fast5 files can contain:

+ Raw signal data
+ Run metadata
+ fastq-basecalls
+ Other additional analyses

The Fast5 file format is an implementation of the HDF5 file format specifically for ONT sequencing data. For more information on Fast5 and a tool to interface with these types of file please see:
https://github.com/nanoporetech/ont_fast5_api

### Summary file

The MinION and GridION output a single sequencing summary file in addition to the Fast5 files. This file contains metdata which descirbes each sequenced read.

We will not use Fast5 or summary files for this tutorial as they are not needed most of the time. It is likely these files will not be intially provided to you by a sequencing service as they are very large. However, if you do require them you can always ask but be careful with how long the sequencing centre will retain your data.

### BAM file
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/bam.png", auto_pdf = TRUE)
``` 

BAM files (.bam) are the binary version of SAM files (.sam). This means:

- BAM files are not human readable whilst SAM files are.
- SAM files are larger than BAM.

Generally programs that can work on SAM files can also work on BAM files. Due to the size difference it is preferable to store data in BAM format over SAM. Even though a BAM file is smaller than their matching SAM file, BAM files are still very large and can be >100GB.

SAM stands for \"Sequence Alignment/Map\" (the B in BAM is Binary). SAM files are tab delimited and contain alignment information. 

It can be useful to contain unaligned reads from sequencing machines (e.g. PacBio and ONT) in BAM files as they can contain more metadata in the header and per-record auxiliary tags compared to fastq files. 

If you are working with SAM/BAM files the following link will prove useful: 
https://www.htslib.org/

For more information on the SAM and BAM format please see: 
https://samtools.github.io/hts-specs/SAMv1.pdf


### fastq file
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/fastq.png", auto_pdf = TRUE)
``` 

The fastq file format is very consistent and so there is no real difference between fastq files for Illumina, PacBio, and ONT data.

All fastq files will contain a number of lines divisible by 4. This is because each entry/sequence will take up four lines consisting of the following information:

1. Header for fastq entry known as the fastq header. 
    + This always begins with a __\'@\'__
    + This is where you might see the most difference between different data. Different machines and different public databases will use different formats for fastq headers.
2. Sequence content
3. Quality header 
    + Always begins with a \'+\'. Sometimes also contains the same information as fastq header.  
4. Quality
    + Each base in the 2nd line will have a corresponding quality value in this line.
    + Note this uses Phred encoding, of which there are different formats. Most, if not all, new files will use Phred+33 but be careful if you are using older data as it may use a different one. See the \"Encoding\" section in the following link for more info: https://en.wikipedia.org/wiki/FASTQ_format.
    +  NOTE: __\'@\'__ can be used as a quality value.

An example of the information of one sequence/entry is:

__\@Sequence 1__  
__CTGTTAAATACCGACTTGCGTCAGGTGCGTGAACAACTGGGCCGCTTT__  
__+__  
__=<<<=>\@\@\@ACDCBCDAC\@BAA\@BA\@BBCBBDA\@BB\@>CD\@A\@B?B\@\@__

## Basecalling
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/guppy.png", auto_pdf = TRUE)
``` 

In short, basecalling is the converting of ONT signals to bases and quality. This step converts the Fast5 files to BAM and/or fastq files.

There are many tools to carry out basecalling with ONT data. ONT sequencing machines carry this out with in built programs. However, if you are interested in the tools used for this the primary ones are Guppy and Albacore. Unfortunately it is quite hard to find information about these tools unless you own an ONT machine.

Basecalling with Guppy tutorial:

https://denbi-nanopore-training-course.readthedocs.io/en/latest/basecalling/basecalling.html

Basecalling with Albacore tutorial:

https://denbi-nanopore-training-course.readthedocs.io/en/stable/basecalling/basecalling.html

## QC of fastq file
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/qc_inspection.png", auto_pdf = TRUE)
``` 

For this tutorial we will carry out quality control and checking on fastq files. Some QC steps can be carried out on summary files and/or BAM files. However we will be using fastq files for a variety of reasons:

- Fastq files are the smallest and so these processes will run quicker and the files will take up less storage. Ideal for a tutorial.
- Fastq files are the easiest to work with.
- You will most likely use fastq files in the future for ONT data. 
  - If you get your data sequenced by a genomic centre they will most likely give you your data in demultiplexxed (one fastq file per sample) fastq files.
  - ONT machines generally come with in built basecalling and so will most likely provide fastq files as well as the other formats now.

Before carrying out any specific commands we will first move into the relevant directory.

```{bash eval=FALSE}
cd ~/QC_workshop/ONT_QC/
```

We need to initialise the environment to use the programs we need. This needs to be done in each terminal you want to run the below commands in. Each terminal will only remember what you have told that terminal.

```{bash eval=FALSE}
. usenanopack-1.1.0
```

Look in the directory called data and you will notice there are a few directories. You can see the contents of all these directories with the below command.

```{bash eval=FALSE}
ls data/*
```

Each directory has one fastq file. ONT data likes to be organised with data for one sample being in one directory.

To start with we will only use the fastq file within the directory called \"Acinetobacter\". As you may have figured out, this contains ONT sequencing data of an Acinetobacter genome. Specifically the data is a subset of the SRA (Sequence Read Archive) Run: SRR7119550.

### NanoStat
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/metrics.png", auto_pdf = TRUE)
``` 

The first step is to acquire stats for the sequences in our fastq file. We will use NanoStat (https://github.com/wdecoster/nanostat).

NanoStat is one of the many tools contained in the NanoPack suite (https://github.com/wdecoster/nanopack). We will also use the tools NanoPlot and NanoFilt downstream.

We want to have a tidy set of directories at the end of this analysis. It would be an untidy mess if we had all the output files in the current directory. We will therefore be making directories and subdirectories to send our output.

```{bash eval=FALSE}
mkdir nanostats
```

Finally we will now run NanoStat. The options used are:

- `-n` : File name/path for the output.
- `-t` : Number of threads the script can use.
- `--fastq` : Input data is in fastq format. Other options that can be used are `--fasta`, `--summary`, and `--bam`.

```{bash eval=FALSE}
NanoStat -n nanostats/Acinetobacter_nanostats.tsv \
-t 4 --fastq ./data/Acinetobacter/Acinetobacter_genome_ONT.fastq
```

Now we can look at the output text file. In this case we will use the convenient `less` command.

```{bash eval=FALSE}
less nanostats/Acinetobacter_nanostats.tsv
```

#### NanoStat output format

The file contains four sections with informative headers. These are:

1. __General summary__
   - A list of general summary metrics.
2. __Number, percentage and megabases of reads above quality cutoffs__
   - Based on the mean quality value of a read.
   - The \">Q5\" line shows the number and % of reads with a mean quality above Q5. It also shows the total amount of megabases these reads contain.
3. __Top 5 highest mean basecall quality scores and their read lengths__
   - Shows the top 5 reads with the highest mean quality scores.
4. __Top 5 longest reads and their mean basecall quality score__
   - Shows the top 5 longest reads.

### Porechop
```{r, fig.align = 'center',out.width= '60%', echo=FALSE }
knitr::include_graphics(path = "figures/porechop_logo_knife.png", auto_pdf = TRUE)
``` 

Porechop is a tool to find and remove adapters from ONT data (https://github.com/rrwick/Porechop). Adapters are artificial sequences essential for sequencing but of no biological value and so you will typically want them removed.

Porechop is no longer supported but it still works and there is no alternative for adapter removal. Depending on which basecaller was used on your data, adapter removal may have already been carried out. However, it is always best to run porechop if you are not sure. 

Porechop has a list of known adapters it will look for and remove. These contain:

- Ligation kit adapters
- Rapid kit adapters
- PCR kit adapters
- Barcodes
- Native barcoding
- Rapid barcoding

Porechop will look for these adapters at the start and end of each read. Then it will look for adapters within the sequence (known as middle adapters to Porechop). If it finds a middle adapter it will conclude that the read is chimeric (a recombinant read containing sequence from 2 or more reads) and split the read. Depending on the number of middle adpaters the chimeric read may split into 2 or more reads.

With all that explanation we will now run Porechop. Thankfully the command is relatively straight forward with the options:

- `-t`: Number of threads to be used.
- `-i`: Input path of a fasta file, fastq file, or a directory. If a directory is specified it will be recursively searched for fastq files.
- `-o`: Output path. This will either be a fastq or fasta file name.

```{bash eval=FALSE}
#Create output directory
mkdir porechop
#Run porechop command
porechop -t 4 -i ./data/Acinetobacter/Acinetobacter_genome_ONT.fastq \
-o porechop/Acinetobacter.porechop.fastq
```

Porechop will take a while to run. Whilst it is running, look at the screen output to get an idea of what it is doing. When finished, look at the bottom of the printed results.

Questions:

- How many reads had adapters trimmed from their start?
- How many bases were removed by adapters being trimmed from the end of reads?
- How many reads were split based on middle adpaters?

For more uses of Porechop please see the below links:

- https://github.com/rrwick/Porechop#quick-usage-examples
- https://github.com/rrwick/Porechop#full-usage

### NanoPlot
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/nanoplot_hexes.png", auto_pdf = TRUE)
``` 

NanoPlot can be thought of as the FastQC for ONT data. It produces a lot of useful visualisations to investigate the quality of ONT sequencing data. It can be used for fastq, fasta, bam, and sequencing summary files. The link for its github page is: https://github.com/wdecoster/NanoPlot

ONT data has much lower quality scores than Illumina with Q10 being good. If you have enough coverage and length the low quality can be corrected by downstream processes not covered in this tutorial.

Prior to running NanoPlot we will make a directory for the NanoPlot output. As NanoPlot creates a lot of files, we\'ll make a subdirectory for the NanoPlot output for the porechopped data.

```{bash eval=FALSE}
mkdir nanoplot
mkdir nanoplot/porechop
```

Now to run NanoPlot. The options we will use are:

- `-t`: Number of threads to be used.
- `--fastq`: Specifies the input path which is a fastq file.
- `-o`: Directory where the output will be created.
- `-p`: Prefix of output files. It is useful to have "_" at the end of the prefix.
- `--plots`: Specifies what type of bivariate plot is to be created (more on this later). I find `hex` to be the best. 

```{bash eval=FALSE}
NanoPlot -t 4 \
--fastq porechop/Acinetobacter.porechop.fastq \
-o nanoplot/porechop -p Acinetobacter_ \
--plots hex 
```

You may get the below warning. This is fine and can be ignored.

```{bash, eval = FALSE}
/pub37/matt/programs_chos_8/anaconda3/lib/python3.7/_collections_abc.py:702: MatplotlibDeprecationWarning: The global colormaps dictionary is no longer considered public API.
  return len(self._mapping)
/pub37/matt/programs_chos_8/anaconda3/lib/python3.7/_collections_abc.py:720: MatplotlibDeprecationWarning: The global colormaps dictionary is no longer considered public API.
  yield from self._mapping
```

List the files in the output directory.

```{bash, eval=FALSE}
ls nanoplot/porechop/
```

There are quite a few files. These should all start with \"Acinetobacter_\" thanks to the `-p` option. To quickly check all the results we can open the report html file with firefox.

```{bash eval=FALSE}
firefox nanoplot/porechop/Acinetobacter_NanoPlot-report.html
```

#### NanoPlot output format

The first section contains NanoStat output. Quickly look over this and see how it compares to the NanoStat output of the pre-porechopped reads.

Tip: To open a new terminal, right click VNC background -> Applications -> Shells -> Bash.

After the __Summary Statistics__ section there is a __Plots__ section. This contains the plots:

- __Histogram of read lengths__
   - Histogram of \"Number of _reads_\" (y) against \"Read length\" (x).
- __Histogram of read lengths after log transformations__
   - Histogram of \"Number of _reads_\" (y) against log transformed \"Read length\" (x).
- __Weighted Histogram of read lengths__
   - Histogram of \"Number of _bases_\" (y) against \"Read length\" (x).
- __Weighted Histogram of read lengths after log transformation__
   - Histogram of \"Number of _bases_\" (y) against log transformed \"Read length\" (x).
- __Dynamic histogram of Read length__
   - An interactive Histogram of \"Number of reads\" (y) against \"Read length\" (x) produced by plotly (https://plotly.com/).
   - You can zoom into areas by clicking and dragging. 
      - Make boxes to zoom into a specific area. 
      - Click and drag left or right only to zoom into a specific part of the x axis. 
      - Click and drag up or down only to zoom into a specific part of the y axis. 
      - Click the home icon on the top right to reset the axes.
- __Yield by length__
   - Plot showing the \"Cumulative yield for minimal length\" (y) by \"Read length\" (x).
   - The cumulative yield is measured in Gigabases (billion bases).
   - This plot is useful to know how many bases you would retain if you filtered reads based on read length.
- __Read lengths vs Average read quality plot using hexagonal bins__
   - This is the most informative plot. It is the bivariate plot we indicated we wanted as hexagonal with the option `--plots hex`.
   - Each hex is a bin with a darker colour representing more reads.
   - At the top of the plot is a histogram of Number of reads against read lengths.
   - At the right of the plot is a sideways histogram of number of reads against average read quality.
   
Questions to be answered with the html file:

- How long is the longest read?
- What is the highest mean basecall quality score of a read?
- How many reads have a mean basecall quality score >Q10?
- Approximately where is the highest density of reads in terms of read lengths and average read quality.
- Roughly, do the longer reads have relatively high, medium, or low mean qualities?

Using different input files will give you different plots. For more details please see the \"Plots Generated\" section on: https://github.com/wdecoster/NanoPlot.

### NanoFilt
```{r, fig.align = 'center',out.width= '10%', echo=FALSE }
knitr::include_graphics(path = "figures/quality_filtering.png", auto_pdf = TRUE)
```

NanoFilt can be used to remove/filter reads by quality and/or read length (https://github.com/wdecoster/nanofilt). This is very useful as you will most likely want long and good quality reads for downstream processes, such as genome assemblies.

It is always important to know what your data is and if your planned filtering and trimming is appropriate. For example, you may be working with amplicon data where the read lengths will vary between 500bp and 750bp. In that case it is useful to set a min length of 500 and a maxlength of 750.

This data is from a genome so wanting long and high quality reads is appropriate. Overall quality looks good. The main exception is the low amount of sequencing/coverage. The low coverage is because it is a subset of the whole data for workshop purposes (the full dataset would take too long to run through). There are some shorter length and lower quality reads which we will remove.

First we will create an output directory.
```{bash eval=FALSE}
mkdir nanofilt
```

We will filter out sequences that are shorter than 500bp (`-l 500bp`) and filter out sequences with a average read quality less than Q10 (`-q 10`). We have chosen these values as they are a good default for genomic data. Q10 appeared to be a good choice from NanoPlot as the majority of reads had a quality of >Q10.

```{bash eval=FALSE}
cat porechop/Acinetobacter.porechop.fastq | \
NanoFilt -l 500 -q 10 > \
nanofilt/Acinetobacter.nanofiltq10minlen500.porechop.fastq
```

Are the chosen options appropriate? Let\'s find out.

### Final check
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/finish.png", auto_pdf = TRUE)
```

After trimming or filtering reads (quality control) it is always important to carry out a quality check. We will therefore run NanoPlot again.

__Note__: We are using a long informative output directory name. This is important as we may need to rerun NanoFilt a few more times with different options until we are happy. 

```{bash eval=FALSE}
#Make an output directory before running NanoPlot
mkdir nanoplot/nanofiltq10minlen500_porechop
#Run NanoPlot on the fastq file with the filtered and porechopped data
NanoPlot -t 4 \
--fastq nanofilt/Acinetobacter.nanofiltq10minlen500.porechop.fastq \
-o nanoplot/nanofiltq10minlen500_porechop/ -p Acinetobacter_ \
--plots hex 
```

Now inspect the report

```{bash, eval=FALSE}
firefox nanoplot/nanofiltq10minlen500_porechop/Acinetobacter_NanoPlot-report.html
```

Ultimately when quality checking we need to take into account how much data we are left with. We need to make sure we have a good amount of bases and reads for our application. However, we also want to make sure we aren't left with a lot of poor quality data.

Depending on our application the quality of the reads may be less important, or the other way around. The is the same with the amount of reads and bases. What is needed will be clearer when you know what type of data you have. These considerations will be covered in our future workshops of the specific data types

A quick example is data for a genome assembly. For this type of analysis we will want our number of bases to reach a certain coverage. We\'ll want a decent quality but a high coverage can help overcome lower quality (more on this in a future workshop). Generally for ONT assemblies you will want 20X - 100X coverage with a higher coverage producing a better assembly (100x coverage= 100bp sequencing data per 1bp of the genome size). Higher coverages may be worse as it can be too much information for an assembly algorithm to cope with.

In addition to the amount of bases, the length of reads is also important. One read that is 10kbp long is most likely better than 10 reads that are each 1kbp long for a genome assembly. This is because less reads will need to be assembled.

Keeping all this in mind, let\'s say this sequencing is for a genome with a size of 0.5 Mbp (0.5 million base pairs). We want at least 20X coverage (i.e 10,000,000.0 total bases). Do we have this for the data that was porechopped and nanofiltered?

If not, try running NanoFilt with a different choices for `--q` until you do. 

+ What is an option that works to get the desired coverage? 
+ Is the overall quality decent (Mean & Median read quality > 10)?
+ What is the N50 of your QC\'d reads?
+ How many extra reads and bases did you retain compared to the Q10 filtering?

Tips:

+ Make sure your new output paths have unique names.
+ Use NanoPlot results to compare the Q10 filtering to your own. The NanoStat part is especially helpful.

<!--chapter:end:04-ONT.Rmd-->

# Practice
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/weight.png", auto_pdf = TRUE)
``` 

This section contains information on more data that you can carry out QC (Quality control and checking) as self guided practice.

## ONT data
```{r, fig.align = 'center',out.width= '60%', echo=FALSE }
knitr::include_graphics(path = "figures/ONT_logo.png", auto_pdf = TRUE)
``` 
The extra data consists of 2 subsetted runs from Bio Project: PRJNA477342 (https://www.ncbi.nlm.nih.gov/bioproject/?term=prjna477342).

The data sets are:

+ Flavobacterium
   + Genomic data for a __Flavobacterium__ isolate.
   + Original data from: SRA run SRR7449788.
+ Pseudonocardia
   + Genomic data for a __Pseudonocardia__ isolate.
   + Original data from: SRA run SRR7447112.
   
Run through all the ONT QC steps for these two datasets. Use all the same parameters and options, making sure to change input and output file paths and prefixes where appropriate.

__Tips__: 

+ If you are editing commands you previously ran it is best to change the outputs first so you don\'t accidentally overwrite previous results.
+ You can run the command `firefox nanoplot/*/*NanoPlot*html`. This will open all the NanoPlot report htmls in one firefox window. There will be a separate tab for each report.

__Questions__:

1. Which of the three datasets (Acinetobacter, Flavobacterium, and Pseudocardia) has the highest initial (pre porechopping) mean read quality (this may be more than one)?
2. Which of the three datasets has the highest initial mean read length?
3. Which of the three datasets has the longest read in the initial data? How long is the read and what is it mean basecall quality score?
4. Answer questions 1-3 for the porechopped data. Are the results of NanoStats significantly changed for any of the samples?
5. Look at the NanoPlot results for the porechopped and NanoFiltered data using `-l 500` and `-q 10`.
   + How many bases and reads are removed from the samples due to NanoFilt?
   + Are the parameters too stringent for some of the samples (losing too much data)?
   + Try out some different parameters until you are happy with the results.
   
Ultimately, knowing what is good and poor data takes experience and practice. The `-l 500` and `-q10` is a good default for ONT genomic data and may be good for other uses. As with all defaults you may need to change these for your particular data.

Hopefully that has been good practice and you are ready to QC your own data.


## PacBio data

```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/Pacbio_logo.png", auto_pdf = TRUE)
``` 

The extra data sets are from whole genome sequencing projects for three microbial isolates. These data are a subset of a multiplexed microbial sequencing run of 15 microbial isolates, available here: https://github.com/PacificBiosciences/DevNet/wiki/Microbial-Multiplexing-Data-Set---48-plex:-PacBio-Sequel-II-System,-Chemistry-v2.0,-SMRT-Link-v8.0-Analysis

The data sets are:

- _Helicobacter pylori_ in the file \"bc1018.subreads.bam\"
- _Methanocorpusculum labreanum_ in the file \"bc1037.subreads.bam\"
- _Neisseria meningitidis_ in the file \"bc1044.subreads.bam\"

These files are in the directory `~/QC_workshop/PB_QC/data/PB_exercise`

Run through the SequelTools quality check on these three data sets to generate the summary values and read length plots (there is no need to do the filtering step).

__Questions__:

1. Which of the three samples contains the most subread data (Mb)?
2. Which samples has the highest subread N50? And which the lowest?
3. Which sample contains the longest subread, and what is it\'s approximate length?
4. Considering all three samples, do any of them looked to have failed or sequenced very badly compared to the others?

The filtering tool from SequelTools would likely be of limited use here as the longest subread and total subread lengths are very similar, as we would want in a whole genome CLR project. For PacBio data it is common for downstream tools (such as assemblers, or ccs for amplicon data) will apply some filtering and thye require the raw data as input. We will cover specific examples in our future workshops that use PacBio sequences.

Hopefully that has been good practice and you are ready to QC your own data.

<!--chapter:end:05-Practice.Rmd-->

# Further resources
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/further_resources.png", auto_pdf = TRUE)
``` 


## Long read sequencing resources
```{r, fig.align = 'center',out.width= '15%', echo=FALSE }
knitr::include_graphics(path = "figures/journal.png", auto_pdf = TRUE)
``` 

__Comparison of the two up-to-date sequencing technologies for genome assembly: HiFi reads of Pacific Biosciences Sequel II system and ultralong reads of Oxford Nanopore__

https://academic.oup.com/gigascience/article/9/12/giaa123/6034784

__Opportunities and challenges in long-read sequencing data analysis__

https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1935-5

## PacBio resources
```{r, fig.align = 'center',out.width= '40%', echo=FALSE }
knitr::include_graphics(path = "figures/Pacbio_logo.png", auto_pdf = TRUE)
``` 

__Pacbio glossary of terms__

https://www.pacb.com/wp-content/uploads/2015/09/Pacific-Biosciences-Glossary-of-Terms.pdf

__Pacbio Brochure__

https://www.pacb.com/wp-content/uploads/SMRT-Sequencing-Brochure-Delivering-highly-accurate-long-reads-to-drive-discovery-in-life-science.pdf

__PacBio Sequencing 101: Understanding Accuracy in DNA Sequencing__

https://www.pacb.com/blog/understanding-accuracy-in-dna-sequencing/

__Sequel systems__

https://www.pacb.com/products-and-services/sequel-system/


## ONT resources
```{r, fig.align = 'center',out.width= '60%', echo=FALSE }
knitr::include_graphics(path = "figures/ONT_logo.png", auto_pdf = TRUE)
``` 

__Products comparison__

https://nanoporetech.com/products/comparison

__Nanopore accuracy__

https://nanoporetech.com/accuracy

__Adaptive Sampling__

https://nanoporetech.com/about-us/news/towards-real-time-targeting-enrichment-or-other-sampling-nanopore-sequencing-devices

__Blog on history of Nanopore sequencing__

https://www.whatisbiotechnology.org/index.php/science/summary/nanopore/nanopore-sequencing-makes-it-possible-to-decode-the

## Tools
```{r, fig.align = 'center',out.width= '20%', echo=FALSE }
knitr::include_graphics(path = "figures/toolbox.png", auto_pdf = TRUE)
``` 

__LONG-READ-TOOLS__

https://long-read-tools.org/

## Misc

__What\'s N50__

https://www.molecularecologist.com/2017/03/29/whats-n50/

<!--chapter:end:06-Furter_resources.Rmd-->

